{"version":3,"file":"gen-ai-attributes.js","sources":["../../../../src/tracing/ai/gen-ai-attributes.ts"],"sourcesContent":["/**\n * OpenAI Integration Telemetry Attributes\n * Based on OpenTelemetry Semantic Conventions for Generative AI\n * @see https://opentelemetry.io/docs/specs/semconv/gen-ai/\n */\n\n// =============================================================================\n// OPENTELEMETRY SEMANTIC CONVENTIONS FOR GENAI\n// =============================================================================\n\n/**\n * The input messages sent to the model\n */\nexport const GEN_AI_PROMPT_ATTRIBUTE = 'gen_ai.prompt';\n\n/**\n * The Generative AI system being used\n * For OpenAI, this should always be \"openai\"\n */\nexport const GEN_AI_SYSTEM_ATTRIBUTE = 'gen_ai.system';\n\n/**\n * The name of the model as requested\n * Examples: \"gpt-4\", \"gpt-3.5-turbo\"\n */\nexport const GEN_AI_REQUEST_MODEL_ATTRIBUTE = 'gen_ai.request.model';\n\n/**\n * Whether streaming was enabled for the request\n */\nexport const GEN_AI_REQUEST_STREAM_ATTRIBUTE = 'gen_ai.request.stream';\n\n/**\n * The temperature setting for the model request\n */\nexport const GEN_AI_REQUEST_TEMPERATURE_ATTRIBUTE = 'gen_ai.request.temperature';\n\n/**\n * The maximum number of tokens requested\n */\nexport const GEN_AI_REQUEST_MAX_TOKENS_ATTRIBUTE = 'gen_ai.request.max_tokens';\n\n/**\n * The frequency penalty setting for the model request\n */\nexport const GEN_AI_REQUEST_FREQUENCY_PENALTY_ATTRIBUTE = 'gen_ai.request.frequency_penalty';\n\n/**\n * The presence penalty setting for the model request\n */\nexport const GEN_AI_REQUEST_PRESENCE_PENALTY_ATTRIBUTE = 'gen_ai.request.presence_penalty';\n\n/**\n * The top_p (nucleus sampling) setting for the model request\n */\nexport const GEN_AI_REQUEST_TOP_P_ATTRIBUTE = 'gen_ai.request.top_p';\n\n/**\n * The top_k setting for the model request\n */\nexport const GEN_AI_REQUEST_TOP_K_ATTRIBUTE = 'gen_ai.request.top_k';\n\n/**\n * Stop sequences for the model request\n */\nexport const GEN_AI_REQUEST_STOP_SEQUENCES_ATTRIBUTE = 'gen_ai.request.stop_sequences';\n\n/**\n * The encoding format for the model request\n */\nexport const GEN_AI_REQUEST_ENCODING_FORMAT_ATTRIBUTE = 'gen_ai.request.encoding_format';\n\n/**\n * The dimensions for the model request\n */\nexport const GEN_AI_REQUEST_DIMENSIONS_ATTRIBUTE = 'gen_ai.request.dimensions';\n\n/**\n * Array of reasons why the model stopped generating tokens\n */\nexport const GEN_AI_RESPONSE_FINISH_REASONS_ATTRIBUTE = 'gen_ai.response.finish_reasons';\n\n/**\n * The name of the model that generated the response\n */\nexport const GEN_AI_RESPONSE_MODEL_ATTRIBUTE = 'gen_ai.response.model';\n\n/**\n * The unique identifier for the response\n */\nexport const GEN_AI_RESPONSE_ID_ATTRIBUTE = 'gen_ai.response.id';\n\n/**\n * The reason why the model stopped generating tokens\n */\nexport const GEN_AI_RESPONSE_STOP_REASON_ATTRIBUTE = 'gen_ai.response.stop_reason';\n\n/**\n * The number of tokens used in the prompt\n */\nexport const GEN_AI_USAGE_INPUT_TOKENS_ATTRIBUTE = 'gen_ai.usage.input_tokens';\n\n/**\n * The number of tokens used in the response\n */\nexport const GEN_AI_USAGE_OUTPUT_TOKENS_ATTRIBUTE = 'gen_ai.usage.output_tokens';\n\n/**\n * The total number of tokens used (input + output)\n */\nexport const GEN_AI_USAGE_TOTAL_TOKENS_ATTRIBUTE = 'gen_ai.usage.total_tokens';\n\n/**\n * The operation name\n */\nexport const GEN_AI_OPERATION_NAME_ATTRIBUTE = 'gen_ai.operation.name';\n\n/**\n * Original length of messages array, used to indicate truncations had occured\n */\nexport const GEN_AI_INPUT_MESSAGES_ORIGINAL_LENGTH_ATTRIBUTE = 'sentry.sdk_meta.gen_ai.input.messages.original_length';\n\n/**\n * The prompt messages\n * Only recorded when recordInputs is enabled\n */\nexport const GEN_AI_INPUT_MESSAGES_ATTRIBUTE = 'gen_ai.input.messages';\n\n/**\n * The system instructions extracted from system messages\n * Only recorded when recordInputs is enabled\n * According to OpenTelemetry spec: https://opentelemetry.io/docs/specs/semconv/registry/attributes/gen-ai/#gen-ai-system-instructions\n */\nexport const GEN_AI_SYSTEM_INSTRUCTIONS_ATTRIBUTE = 'gen_ai.system_instructions';\n\n/**\n * The response text\n * Only recorded when recordOutputs is enabled\n */\nexport const GEN_AI_RESPONSE_TEXT_ATTRIBUTE = 'gen_ai.response.text';\n\n/**\n * The available tools from incoming request\n * Only recorded when recordInputs is enabled\n */\nexport const GEN_AI_REQUEST_AVAILABLE_TOOLS_ATTRIBUTE = 'gen_ai.request.available_tools';\n\n/**\n * Whether the response is a streaming response\n */\nexport const GEN_AI_RESPONSE_STREAMING_ATTRIBUTE = 'gen_ai.response.streaming';\n\n/**\n * The tool calls from the response\n * Only recorded when recordOutputs is enabled\n */\nexport const GEN_AI_RESPONSE_TOOL_CALLS_ATTRIBUTE = 'gen_ai.response.tool_calls';\n\n/**\n * The agent name\n */\nexport const GEN_AI_AGENT_NAME_ATTRIBUTE = 'gen_ai.agent.name';\n\n/**\n * The pipeline name\n */\nexport const GEN_AI_PIPELINE_NAME_ATTRIBUTE = 'gen_ai.pipeline.name';\n\n/**\n * The conversation ID for linking messages across API calls\n * For OpenAI Assistants API: thread_id\n * For LangGraph: configurable.thread_id\n */\nexport const GEN_AI_CONVERSATION_ID_ATTRIBUTE = 'gen_ai.conversation.id';\n\n/**\n * The number of cache creation input tokens used\n */\nexport const GEN_AI_USAGE_CACHE_CREATION_INPUT_TOKENS_ATTRIBUTE = 'gen_ai.usage.cache_creation_input_tokens';\n\n/**\n * The number of cache read input tokens used\n */\nexport const GEN_AI_USAGE_CACHE_READ_INPUT_TOKENS_ATTRIBUTE = 'gen_ai.usage.cache_read_input_tokens';\n\n/**\n * The number of cache write input tokens used\n */\nexport const GEN_AI_USAGE_INPUT_TOKENS_CACHE_WRITE_ATTRIBUTE = 'gen_ai.usage.input_tokens.cache_write';\n\n/**\n * The number of cached input tokens that were used\n */\nexport const GEN_AI_USAGE_INPUT_TOKENS_CACHED_ATTRIBUTE = 'gen_ai.usage.input_tokens.cached';\n\n/**\n * The span operation name for invoking an agent\n */\nexport const GEN_AI_INVOKE_AGENT_OPERATION_ATTRIBUTE = 'gen_ai.invoke_agent';\n\n/**\n * The span operation name for generating text\n */\nexport const GEN_AI_GENERATE_TEXT_DO_GENERATE_OPERATION_ATTRIBUTE = 'gen_ai.generate_text';\n\n/**\n * The span operation name for streaming text\n */\nexport const GEN_AI_STREAM_TEXT_DO_STREAM_OPERATION_ATTRIBUTE = 'gen_ai.stream_text';\n\n/**\n * The span operation name for generating object\n */\nexport const GEN_AI_GENERATE_OBJECT_DO_GENERATE_OPERATION_ATTRIBUTE = 'gen_ai.generate_object';\n\n/**\n * The span operation name for streaming object\n */\nexport const GEN_AI_STREAM_OBJECT_DO_STREAM_OPERATION_ATTRIBUTE = 'gen_ai.stream_object';\n\n/**\n * The embeddings input\n * Only recorded when recordInputs is enabled\n */\nexport const GEN_AI_EMBEDDINGS_INPUT_ATTRIBUTE = 'gen_ai.embeddings.input';\n\n/**\n * The span operation name for embedding\n */\nexport const GEN_AI_EMBED_DO_EMBED_OPERATION_ATTRIBUTE = 'gen_ai.embed';\n\n/**\n * The span operation name for embedding many\n */\nexport const GEN_AI_EMBED_MANY_DO_EMBED_OPERATION_ATTRIBUTE = 'gen_ai.embed_many';\n\n/**\n * The span operation name for executing a tool\n */\nexport const GEN_AI_EXECUTE_TOOL_OPERATION_ATTRIBUTE = 'gen_ai.execute_tool';\n\n/**\n * The tool name for tool call spans\n */\nexport const GEN_AI_TOOL_NAME_ATTRIBUTE = 'gen_ai.tool.name';\n\n/**\n * The tool call ID\n */\nexport const GEN_AI_TOOL_CALL_ID_ATTRIBUTE = 'gen_ai.tool.call.id';\n\n/**\n * The tool type (e.g., 'function')\n */\nexport const GEN_AI_TOOL_TYPE_ATTRIBUTE = 'gen_ai.tool.type';\n\n/**\n * The tool input/arguments\n */\nexport const GEN_AI_TOOL_INPUT_ATTRIBUTE = 'gen_ai.tool.input';\n\n/**\n * The tool output/result\n */\nexport const GEN_AI_TOOL_OUTPUT_ATTRIBUTE = 'gen_ai.tool.output';\n\n// =============================================================================\n// OPENAI-SPECIFIC ATTRIBUTES\n// =============================================================================\n\n/**\n * The response ID from OpenAI\n */\nexport const OPENAI_RESPONSE_ID_ATTRIBUTE = 'openai.response.id';\n\n/**\n * The response model from OpenAI\n */\nexport const OPENAI_RESPONSE_MODEL_ATTRIBUTE = 'openai.response.model';\n\n/**\n * The response timestamp from OpenAI (ISO string)\n */\nexport const OPENAI_RESPONSE_TIMESTAMP_ATTRIBUTE = 'openai.response.timestamp';\n\n/**\n * The number of completion tokens used\n */\nexport const OPENAI_USAGE_COMPLETION_TOKENS_ATTRIBUTE = 'openai.usage.completion_tokens';\n\n/**\n * The number of prompt tokens used\n */\nexport const OPENAI_USAGE_PROMPT_TOKENS_ATTRIBUTE = 'openai.usage.prompt_tokens';\n\n// =============================================================================\n// OPENAI OPERATIONS\n// =============================================================================\n\n/**\n * OpenAI API operations following OpenTelemetry semantic conventions\n * @see https://opentelemetry.io/docs/specs/semconv/gen-ai/gen-ai-spans/#llm-request-spans\n */\nexport const OPENAI_OPERATIONS = {\n  CHAT: 'chat',\n  EMBEDDINGS: 'embeddings',\n} as const;\n\n// =============================================================================\n// ANTHROPIC AI OPERATIONS\n// =============================================================================\n\n/**\n * The response timestamp from Anthropic AI (ISO string)\n */\nexport const ANTHROPIC_AI_RESPONSE_TIMESTAMP_ATTRIBUTE = 'anthropic.response.timestamp';\n"],"names":[],"mappings":"AAAA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACO,MAAM,uBAAA,GAA0B;;AAEvC;AACA;AACA;AACA;AACO,MAAM,uBAAA,GAA0B;;AAEvC;AACA;AACA;AACA;AACO,MAAM,8BAAA,GAAiC;;AAE9C;AACA;AACA;AACO,MAAM,+BAAA,GAAkC;;AAE/C;AACA;AACA;AACO,MAAM,oCAAA,GAAuC;;AAEpD;AACA;AACA;AACO,MAAM,mCAAA,GAAsC;;AAEnD;AACA;AACA;AACO,MAAM,0CAAA,GAA6C;;AAE1D;AACA;AACA;AACO,MAAM,yCAAA,GAA4C;;AAEzD;AACA;AACA;AACO,MAAM,8BAAA,GAAiC;;AAE9C;AACA;AACA;AACO,MAAM,8BAAA,GAAiC;;AAO9C;AACA;AACA;AACO,MAAM,wCAAA,GAA2C;;AAExD;AACA;AACA;AACO,MAAM,mCAAA,GAAsC;;AAEnD;AACA;AACA;AACO,MAAM,wCAAA,GAA2C;;AAExD;AACA;AACA;AACO,MAAM,+BAAA,GAAkC;;AAE/C;AACA;AACA;AACO,MAAM,4BAAA,GAA+B;;AAE5C;AACA;AACA;AACO,MAAM,qCAAA,GAAwC;;AAErD;AACA;AACA;AACO,MAAM,mCAAA,GAAsC;;AAEnD;AACA;AACA;AACO,MAAM,oCAAA,GAAuC;;AAEpD;AACA;AACA;AACO,MAAM,mCAAA,GAAsC;;AAEnD;AACA;AACA;AACO,MAAM,+BAAA,GAAkC;;AAE/C;AACA;AACA;AACO,MAAM,+CAAA,GAAkD;;AAE/D;AACA;AACA;AACA;AACO,MAAM,+BAAA,GAAkC;;AAE/C;AACA;AACA;AACA;AACA;AACO,MAAM,oCAAA,GAAuC;;AAEpD;AACA;AACA;AACA;AACO,MAAM,8BAAA,GAAiC;;AAE9C;AACA;AACA;AACA;AACO,MAAM,wCAAA,GAA2C;;AAExD;AACA;AACA;AACO,MAAM,mCAAA,GAAsC;;AAEnD;AACA;AACA;AACA;AACO,MAAM,oCAAA,GAAuC;;AAEpD;AACA;AACA;AACO,MAAM,2BAAA,GAA8B;;AAE3C;AACA;AACA;AACO,MAAM,8BAAA,GAAiC;;AAE9C;AACA;AACA;AACA;AACA;AACO,MAAM,gCAAA,GAAmC;;AAEhD;AACA;AACA;AACO,MAAM,kDAAA,GAAqD;;AAElE;AACA;AACA;AACO,MAAM,8CAAA,GAAiD;;AAE9D;AACA;AACA;AACO,MAAM,+CAAA,GAAkD;;AAE/D;AACA;AACA;AACO,MAAM,0CAAA,GAA6C;;AAE1D;AACA;AACA;AACO,MAAM,uCAAA,GAA0C;;AAEvD;AACA;AACA;AACO,MAAM,oDAAA,GAAuD;;AAEpE;AACA;AACA;AACO,MAAM,gDAAA,GAAmD;;AAEhE;AACA;AACA;AACO,MAAM,sDAAA,GAAyD;;AAEtE;AACA;AACA;AACO,MAAM,kDAAA,GAAqD;;AAElE;AACA;AACA;AACA;AACO,MAAM,iCAAA,GAAoC;;AAEjD;AACA;AACA;AACO,MAAM,yCAAA,GAA4C;;AAEzD;AACA;AACA;AACO,MAAM,8CAAA,GAAiD;;AAE9D;AACA;AACA;AACO,MAAM,uCAAA,GAA0C;;AAEvD;AACA;AACA;AACO,MAAM,0BAAA,GAA6B;;AAE1C;AACA;AACA;AACO,MAAM,6BAAA,GAAgC;;AAE7C;AACA;AACA;AACO,MAAM,0BAAA,GAA6B;;AAE1C;AACA;AACA;AACO,MAAM,2BAAA,GAA8B;;AAE3C;AACA;AACA;AACO,MAAM,4BAAA,GAA+B;;AAE5C;AACA;AACA;;AAEA;AACA;AACA;AACO,MAAM,4BAAA,GAA+B;;AAE5C;AACA;AACA;AACO,MAAM,+BAAA,GAAkC;;AAE/C;AACA;AACA;AACO,MAAM,mCAAA,GAAsC;;AAEnD;AACA;AACA;AACO,MAAM,wCAAA,GAA2C;;AAExD;AACA;AACA;AACO,MAAM,oCAAA,GAAuC;;AAEpD;AACA;AACA;;AAEA;AACA;AACA;AACA;AACO,MAAM,oBAAoB;AACjC,EAAE,IAAI,EAAE,MAAM;AACd,EAAE,UAAU,EAAE,YAAY;AAC1B,CAAA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACO,MAAM,yCAAA,GAA4C;;;;"}